{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(stringr)\n",
    "library(doMC)\n",
    "library(lfe)\n",
    "library(lubridate)\n",
    "library(ggplot2)\n",
    "library(ggsci)\n",
    "library(xgboost)\n",
    "registerDoMC(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  ds = \u001b[34mcol_date(format = \"\")\u001b[39m,\n",
      "  country = \u001b[31mcol_character()\u001b[39m,\n",
      "  polygon_source = \u001b[31mcol_character()\u001b[39m,\n",
      "  polygon_id = \u001b[31mcol_character()\u001b[39m,\n",
      "  polygon_name = \u001b[31mcol_character()\u001b[39m,\n",
      "  all_day_bing_tiles_visited_relative_change = \u001b[32mcol_double()\u001b[39m,\n",
      "  all_day_ratio_single_tile_users = \u001b[32mcol_double()\u001b[39m,\n",
      "  baseline_name = \u001b[31mcol_character()\u001b[39m,\n",
      "  baseline_type = \u001b[31mcol_character()\u001b[39m\n",
      ")\n",
      "\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  user_county = \u001b[31mcol_character()\u001b[39m,\n",
      "  fr_county = \u001b[31mcol_character()\u001b[39m,\n",
      "  scaled_sci = \u001b[32mcol_double()\u001b[39m,\n",
      "  n = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp  <- '/pool001/mfzhao/'\n",
    "fb  <- read_tsv(str_c(fp, 'mobility/fb/movement-range-2020-08-08.txt'))\n",
    "sg  <- read_rds(str_c(fp, 'PROCESSED_DATA/panel_pre_xgr.RDS'))\n",
    "sci <- read_csv(str_c(fp, 'PROCESSED_DATA/processed_sci.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb %>%\n",
    "    filter(country == 'USA') %>%\n",
    "    select(key = polygon_id, \n",
    "           date = ds, \n",
    "           all_day_bing_tiles_visited_relative_change,\n",
    "           all_day_ratio_single_tile_users) -> fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"key\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sg %>%\n",
    "    select(key) %>%\n",
    "    distinct() %>%\n",
    "    inner_join(fb %>%\n",
    "               group_by(key) %>%\n",
    "               tally() %>%\n",
    "               filter(n == max(n)) %>%\n",
    "               select(key)) -> keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"key\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sg %>% \n",
    "    filter(date >= as.Date('2020-03-01')) %>%\n",
    "    inner_join(keys) %>%\n",
    "    select(!matches('alter')) %>%\n",
    "    arrange(date, key) -> sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"key\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fb %>%\n",
    "    inner_join(keys) %>%\n",
    "    filter(date < as.Date('2020-07-01')) %>%\n",
    "    arrange(date, key) %>%\n",
    "    rename(btvrc = all_day_bing_tiles_visited_relative_change,\n",
    "           rstu  = all_day_ratio_single_tile_users) %>%\n",
    "    mutate(rnstu     = 1 - rstu,\n",
    "           log_rnstu = log(rnstu)) %>%\n",
    "    select(-rstu) -> fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = c(\"key\", \"date\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join(sg, fb) -> df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci %>%\n",
    "    inner_join(keys, by = c('fr_county' = 'key')) %>%\n",
    "    inner_join(keys, by = c('user_county' = 'key')) -> sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci %>%\n",
    "    mutate(user_state = str_sub(user_county, 1, 2),\n",
    "           fr_state   = str_sub(fr_county, 1, 2)) %>%\n",
    "    group_by(user_county) %>%\n",
    "    mutate(w = ifelse(user_state == fr_state, 0, scaled_sci * n),\n",
    "           w = w/sum(w)) %>%\n",
    "    select(key = user_county, fr_county, w) %>%\n",
    "    spread(key = fr_county, value = w) %>%\n",
    "    ungroup(key) %>%\n",
    "    arrange(key) %>%\n",
    "    select(-key) %>%\n",
    "    as.matrix() -> stateWM\n",
    "\n",
    "rownames(stateWM) <- keys$key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedAlters <- function(df, wm, ...) {\n",
    "    df %>% \n",
    "        select(date, key, ...) %>%\n",
    "        spread(key = key, value = ...) %>%\n",
    "        ungroup() %>%\n",
    "        select(-date) %>%\n",
    "        as.matrix() -> txn_data\n",
    "    \n",
    "    df %>%\n",
    "        ungroup() %>%\n",
    "        select(date) %>%\n",
    "        distinct() %>%\n",
    "        arrange(date) -> dates\n",
    "    \n",
    "    outMatrix <- tcrossprod(txn_data, wm)\n",
    "    colnames(outMatrix) <- colnames(txn_data)\n",
    "    \n",
    "    data.frame(dates, outMatrix) %>%\n",
    "        gather(key = 'key', value = 'value', -date) %>%\n",
    "        arrange(date, key) %>%\n",
    "        select(-date, -key) -> out_df\n",
    "    return(out_df$value)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'p1sdp'</li><li>'p2shp'</li><li>'p3rop'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'p1sdp'\n",
       "\\item 'p2shp'\n",
       "\\item 'p3rop'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'p1sdp'\n",
       "2. 'p2shp'\n",
       "3. 'p3rop'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"p1sdp\" \"p2shp\" \"p3rop\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df %>%\n",
    "    select(p1sdp, p2shp, p3rop) %>%\n",
    "    colnames() -> cols_to_alterize\n",
    "cols_to_alterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalters <- foreach(i = 1:length(cols_to_alterize), .combine = cbind) %dopar% \n",
    "    weightedAlters(df, stateWM, cols_to_alterize[i])\n",
    "colnames(stalters) <- str_c('stalter_', cols_to_alterize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df %>%\n",
    "    bind_cols(as.data.frame(stalters)) -> df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"key\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set.seed(2345)\n",
    "df %>%\n",
    "    ungroup() %>%\n",
    "    select(key) %>%\n",
    "    distinct() %>% \n",
    "    mutate(i = sample(1:n(), n(), replace = F),\n",
    "           fold = i %% 3 + 1) %>%\n",
    "    select(-i) -> folds\n",
    "\n",
    "df %>%\n",
    "    inner_join(folds) %>%\n",
    "    arrange(date, key) %>%\n",
    "    ungroup() -> df\n",
    "\n",
    "df %>%\n",
    "    ungroup() %>%\n",
    "    mutate(PRCP.r_fe = felm(PRCP ~ 0 | key + date, ., weights = df$n)$resid,\n",
    "           TMAX.r_fe = felm(TMAX ~ 0 | key + date, ., weights = df$n)$resid) %>%\n",
    "    select(date, key, PRCP.r_fe, TMAX.r_fe, fold, n) -> rdf\n",
    "\n",
    "folds <- list(which(folds$fold %in% 1), \n",
    "              which(folds$fold %in% 2), \n",
    "              which(folds$fold %in% 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGresidualizer <- function(Y, colname) {\n",
    "    print(colname)\n",
    "    rdf %>%\n",
    "        ungroup() %>% \n",
    "        mutate(Y = Y,\n",
    "               Y.r = felm(Y ~ 0 | key + date, ., weights = rdf$n)$resid) -> temp_df\n",
    "  \n",
    "    folds <- list(which(temp_df$fold %in% 1), \n",
    "                  which(temp_df$fold %in% 2), \n",
    "                  which(temp_df$fold %in% 3))\n",
    "\n",
    "    dm    <- xgb.DMatrix(data = model.matrix(~ 0 + PRCP.r_fe + TMAX.r_fe, temp_df), label = temp_df$Y.r)\n",
    "    param <- list(max_depth=2, eta=.5, silent=1, objective='reg:linear')\n",
    "    fit   <- xgb.cv(params = param, \n",
    "                    data = dm, \n",
    "                    folds = folds,\n",
    "                    nrounds = 100, \n",
    "                    early_stopping_rounds = 3, \n",
    "                    weight = temp_df$n)\n",
    "    best_n <- fit$best_iteration\n",
    "    for (i in 1:3) {\n",
    "        tr  <- temp_df %>% filter(fold != i)\n",
    "        trm <- xgb.DMatrix(data = model.matrix(~ 0 + PRCP.r_fe + TMAX.r_fe, tr), label = tr$Y.r)\n",
    "        fit <- xgb.train(params = param, \n",
    "                         data = trm, \n",
    "                         nrounds = best_n, \n",
    "                         weight = tr$n)\n",
    "        te  <- temp_df %>% filter(fold == i)\n",
    "        tem <- xgb.DMatrix(data = model.matrix(~ 0 + PRCP.r_fe + TMAX.r_fe, te), label = te$Y.r)\n",
    "        te %>%\n",
    "            select(date, key) %>%\n",
    "            mutate(pred = predict(fit, newdata = tem)) -> pred_df\n",
    "        assign(str_c('temp',i), pred_df) %>%\n",
    "        select(-pred, -date, -key)\n",
    "    }\n",
    "    out <- bind_rows(temp1, temp2, temp3) %>%\n",
    "        arrange(date, key) %>%\n",
    "        mutate(tempname = temp_df$Y.r - pred) %>%\n",
    "        select(-pred, -date, -key)\n",
    "    \n",
    "    colnames(out) <- str_c(colname, '.r')\n",
    "    return(out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"log_mcbgv\"\n",
      "[1]\ttrain-rmse:0.254527+0.000004\ttest-rmse:0.254533+0.000011 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.133700+0.000010\ttest-rmse:0.133710+0.000025 \n",
      "[3]\ttrain-rmse:0.078341+0.000016\ttest-rmse:0.078357+0.000040 \n",
      "[4]\ttrain-rmse:0.056540+0.000019\ttest-rmse:0.056563+0.000057 \n",
      "[5]\ttrain-rmse:0.049583+0.000020\ttest-rmse:0.049612+0.000068 \n",
      "[6]\ttrain-rmse:0.047666+0.000020\ttest-rmse:0.047699+0.000074 \n",
      "[7]\ttrain-rmse:0.047156+0.000021\ttest-rmse:0.047194+0.000077 \n",
      "[8]\ttrain-rmse:0.047013+0.000019\ttest-rmse:0.047051+0.000079 \n",
      "[9]\ttrain-rmse:0.046973+0.000019\ttest-rmse:0.047011+0.000080 \n",
      "[10]\ttrain-rmse:0.046954+0.000022\ttest-rmse:0.046994+0.000083 \n",
      "[11]\ttrain-rmse:0.046944+0.000025\ttest-rmse:0.046985+0.000079 \n",
      "[12]\ttrain-rmse:0.046940+0.000025\ttest-rmse:0.046981+0.000078 \n",
      "[13]\ttrain-rmse:0.046936+0.000025\ttest-rmse:0.046979+0.000079 \n",
      "[14]\ttrain-rmse:0.046930+0.000025\ttest-rmse:0.046975+0.000077 \n",
      "[15]\ttrain-rmse:0.046925+0.000024\ttest-rmse:0.046969+0.000077 \n",
      "[16]\ttrain-rmse:0.046919+0.000026\ttest-rmse:0.046963+0.000075 \n",
      "[17]\ttrain-rmse:0.046916+0.000025\ttest-rmse:0.046962+0.000076 \n",
      "[18]\ttrain-rmse:0.046909+0.000030\ttest-rmse:0.046955+0.000072 \n",
      "[19]\ttrain-rmse:0.046904+0.000028\ttest-rmse:0.046952+0.000075 \n",
      "[20]\ttrain-rmse:0.046899+0.000028\ttest-rmse:0.046948+0.000076 \n",
      "[21]\ttrain-rmse:0.046895+0.000030\ttest-rmse:0.046944+0.000073 \n",
      "[22]\ttrain-rmse:0.046893+0.000028\ttest-rmse:0.046944+0.000075 \n",
      "[23]\ttrain-rmse:0.046891+0.000028\ttest-rmse:0.046943+0.000074 \n",
      "[24]\ttrain-rmse:0.046889+0.000028\ttest-rmse:0.046943+0.000074 \n",
      "[25]\ttrain-rmse:0.046886+0.000029\ttest-rmse:0.046940+0.000074 \n",
      "[26]\ttrain-rmse:0.046881+0.000029\ttest-rmse:0.046937+0.000074 \n",
      "[27]\ttrain-rmse:0.046877+0.000031\ttest-rmse:0.046934+0.000072 \n",
      "[28]\ttrain-rmse:0.046875+0.000032\ttest-rmse:0.046933+0.000071 \n",
      "[29]\ttrain-rmse:0.046873+0.000033\ttest-rmse:0.046933+0.000070 \n",
      "[30]\ttrain-rmse:0.046872+0.000033\ttest-rmse:0.046932+0.000070 \n",
      "[31]\ttrain-rmse:0.046869+0.000033\ttest-rmse:0.046930+0.000071 \n",
      "[32]\ttrain-rmse:0.046867+0.000032\ttest-rmse:0.046928+0.000071 \n",
      "[33]\ttrain-rmse:0.046864+0.000033\ttest-rmse:0.046927+0.000071 \n",
      "[34]\ttrain-rmse:0.046861+0.000033\ttest-rmse:0.046925+0.000071 \n",
      "[35]\ttrain-rmse:0.046860+0.000032\ttest-rmse:0.046925+0.000071 \n",
      "[36]\ttrain-rmse:0.046859+0.000032\ttest-rmse:0.046926+0.000072 \n",
      "[37]\ttrain-rmse:0.046857+0.000032\ttest-rmse:0.046926+0.000071 \n",
      "Stopping. Best iteration:\n",
      "[34]\ttrain-rmse:0.046861+0.000033\ttest-rmse:0.046925+0.000071\n",
      "\n",
      "[1] \"log_pgt2kmt\"\n",
      "[1]\ttrain-rmse:0.257452+0.000044\ttest-rmse:0.257463+0.000083 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.139189+0.000079\ttest-rmse:0.139196+0.000171 \n",
      "[3]\ttrain-rmse:0.087399+0.000128\ttest-rmse:0.087416+0.000267 \n",
      "[4]\ttrain-rmse:0.068558+0.000158\ttest-rmse:0.068585+0.000342 \n",
      "[5]\ttrain-rmse:0.062953+0.000174\ttest-rmse:0.062987+0.000370 \n",
      "[6]\ttrain-rmse:0.061450+0.000183\ttest-rmse:0.061488+0.000372 \n",
      "[7]\ttrain-rmse:0.061058+0.000183\ttest-rmse:0.061100+0.000375 \n",
      "[8]\ttrain-rmse:0.060950+0.000184\ttest-rmse:0.060993+0.000373 \n",
      "[9]\ttrain-rmse:0.060917+0.000183\ttest-rmse:0.060963+0.000375 \n",
      "[10]\ttrain-rmse:0.060903+0.000181\ttest-rmse:0.060949+0.000378 \n",
      "[11]\ttrain-rmse:0.060895+0.000182\ttest-rmse:0.060942+0.000378 \n",
      "[12]\ttrain-rmse:0.060891+0.000183\ttest-rmse:0.060940+0.000376 \n",
      "[13]\ttrain-rmse:0.060887+0.000182\ttest-rmse:0.060938+0.000377 \n",
      "[14]\ttrain-rmse:0.060880+0.000179\ttest-rmse:0.060935+0.000380 \n",
      "[15]\ttrain-rmse:0.060875+0.000181\ttest-rmse:0.060933+0.000377 \n",
      "[16]\ttrain-rmse:0.060869+0.000183\ttest-rmse:0.060929+0.000375 \n",
      "[17]\ttrain-rmse:0.060866+0.000183\ttest-rmse:0.060929+0.000375 \n",
      "[18]\ttrain-rmse:0.060862+0.000184\ttest-rmse:0.060929+0.000374 \n",
      "[19]\ttrain-rmse:0.060858+0.000187\ttest-rmse:0.060927+0.000370 \n",
      "[20]\ttrain-rmse:0.060854+0.000189\ttest-rmse:0.060924+0.000369 \n",
      "[21]\ttrain-rmse:0.060851+0.000188\ttest-rmse:0.060925+0.000369 \n",
      "[22]\ttrain-rmse:0.060848+0.000187\ttest-rmse:0.060926+0.000369 \n",
      "[23]\ttrain-rmse:0.060845+0.000186\ttest-rmse:0.060924+0.000369 \n",
      "[24]\ttrain-rmse:0.060843+0.000185\ttest-rmse:0.060923+0.000371 \n",
      "[25]\ttrain-rmse:0.060838+0.000186\ttest-rmse:0.060919+0.000368 \n",
      "[26]\ttrain-rmse:0.060834+0.000187\ttest-rmse:0.060917+0.000368 \n",
      "[27]\ttrain-rmse:0.060833+0.000187\ttest-rmse:0.060917+0.000368 \n",
      "[28]\ttrain-rmse:0.060829+0.000188\ttest-rmse:0.060916+0.000367 \n",
      "[29]\ttrain-rmse:0.060827+0.000187\ttest-rmse:0.060915+0.000368 \n",
      "[30]\ttrain-rmse:0.060824+0.000187\ttest-rmse:0.060914+0.000368 \n",
      "[31]\ttrain-rmse:0.060821+0.000188\ttest-rmse:0.060912+0.000366 \n",
      "[32]\ttrain-rmse:0.060819+0.000188\ttest-rmse:0.060911+0.000365 \n",
      "[33]\ttrain-rmse:0.060815+0.000187\ttest-rmse:0.060909+0.000369 \n",
      "[34]\ttrain-rmse:0.060812+0.000187\ttest-rmse:0.060909+0.000369 \n",
      "[35]\ttrain-rmse:0.060811+0.000187\ttest-rmse:0.060910+0.000369 \n",
      "[36]\ttrain-rmse:0.060809+0.000187\ttest-rmse:0.060911+0.000368 \n",
      "[37]\ttrain-rmse:0.060807+0.000187\ttest-rmse:0.060911+0.000368 \n",
      "Stopping. Best iteration:\n",
      "[34]\ttrain-rmse:0.060812+0.000187\ttest-rmse:0.060909+0.000369\n",
      "\n",
      "[1] \"log_pgt1hafh\"\n",
      "[1]\ttrain-rmse:0.257773+0.000023\ttest-rmse:0.257775+0.000046 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.139762+0.000043\ttest-rmse:0.139767+0.000095 \n",
      "[3]\ttrain-rmse:0.088306+0.000070\ttest-rmse:0.088313+0.000149 \n",
      "[4]\ttrain-rmse:0.069685+0.000082\ttest-rmse:0.069699+0.000190 \n",
      "[5]\ttrain-rmse:0.064174+0.000091\ttest-rmse:0.064197+0.000195 \n",
      "[6]\ttrain-rmse:0.062700+0.000096\ttest-rmse:0.062732+0.000196 \n",
      "[7]\ttrain-rmse:0.062315+0.000098\ttest-rmse:0.062352+0.000194 \n",
      "[8]\ttrain-rmse:0.062202+0.000098\ttest-rmse:0.062242+0.000193 \n",
      "[9]\ttrain-rmse:0.062170+0.000099\ttest-rmse:0.062216+0.000190 \n",
      "[10]\ttrain-rmse:0.062155+0.000097\ttest-rmse:0.062201+0.000191 \n",
      "[11]\ttrain-rmse:0.062149+0.000099\ttest-rmse:0.062195+0.000191 \n",
      "[12]\ttrain-rmse:0.062145+0.000099\ttest-rmse:0.062195+0.000190 \n",
      "[13]\ttrain-rmse:0.062139+0.000098\ttest-rmse:0.062190+0.000194 \n",
      "[14]\ttrain-rmse:0.062135+0.000098\ttest-rmse:0.062187+0.000192 \n",
      "[15]\ttrain-rmse:0.062130+0.000096\ttest-rmse:0.062185+0.000194 \n",
      "[16]\ttrain-rmse:0.062126+0.000094\ttest-rmse:0.062184+0.000195 \n",
      "[17]\ttrain-rmse:0.062120+0.000097\ttest-rmse:0.062181+0.000195 \n",
      "[18]\ttrain-rmse:0.062115+0.000096\ttest-rmse:0.062177+0.000195 \n",
      "[19]\ttrain-rmse:0.062112+0.000095\ttest-rmse:0.062174+0.000195 \n",
      "[20]\ttrain-rmse:0.062106+0.000093\ttest-rmse:0.062171+0.000196 \n",
      "[21]\ttrain-rmse:0.062102+0.000092\ttest-rmse:0.062169+0.000198 \n",
      "[22]\ttrain-rmse:0.062098+0.000092\ttest-rmse:0.062167+0.000198 \n",
      "[23]\ttrain-rmse:0.062096+0.000093\ttest-rmse:0.062166+0.000197 \n",
      "[24]\ttrain-rmse:0.062094+0.000094\ttest-rmse:0.062167+0.000197 \n",
      "[25]\ttrain-rmse:0.062091+0.000095\ttest-rmse:0.062165+0.000194 \n",
      "[26]\ttrain-rmse:0.062089+0.000095\ttest-rmse:0.062165+0.000195 \n",
      "[27]\ttrain-rmse:0.062086+0.000094\ttest-rmse:0.062166+0.000196 \n",
      "[28]\ttrain-rmse:0.062085+0.000094\ttest-rmse:0.062166+0.000196 \n",
      "Stopping. Best iteration:\n",
      "[25]\ttrain-rmse:0.062091+0.000095\ttest-rmse:0.062165+0.000194\n",
      "\n",
      "[1] \"log_pnchd\"\n",
      "[1]\ttrain-rmse:0.254724+0.000021\ttest-rmse:0.254728+0.000042 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.134098+0.000043\ttest-rmse:0.134097+0.000083 \n",
      "[3]\ttrain-rmse:0.079049+0.000070\ttest-rmse:0.079057+0.000140 \n",
      "[4]\ttrain-rmse:0.057540+0.000094\ttest-rmse:0.057557+0.000190 \n",
      "[5]\ttrain-rmse:0.050736+0.000108\ttest-rmse:0.050761+0.000215 \n",
      "[6]\ttrain-rmse:0.048868+0.000114\ttest-rmse:0.048897+0.000226 \n",
      "[7]\ttrain-rmse:0.048378+0.000118\ttest-rmse:0.048412+0.000224 \n",
      "[8]\ttrain-rmse:0.048242+0.000120\ttest-rmse:0.048281+0.000222 \n",
      "[9]\ttrain-rmse:0.048202+0.000118\ttest-rmse:0.048241+0.000222 \n",
      "[10]\ttrain-rmse:0.048187+0.000117\ttest-rmse:0.048228+0.000224 \n",
      "[11]\ttrain-rmse:0.048180+0.000115\ttest-rmse:0.048222+0.000227 \n",
      "[12]\ttrain-rmse:0.048175+0.000116\ttest-rmse:0.048218+0.000227 \n",
      "[13]\ttrain-rmse:0.048168+0.000115\ttest-rmse:0.048211+0.000228 \n",
      "[14]\ttrain-rmse:0.048161+0.000117\ttest-rmse:0.048205+0.000225 \n",
      "[15]\ttrain-rmse:0.048153+0.000112\ttest-rmse:0.048199+0.000231 \n",
      "[16]\ttrain-rmse:0.048149+0.000111\ttest-rmse:0.048197+0.000233 \n",
      "[17]\ttrain-rmse:0.048147+0.000111\ttest-rmse:0.048197+0.000232 \n",
      "[18]\ttrain-rmse:0.048140+0.000115\ttest-rmse:0.048192+0.000228 \n",
      "[19]\ttrain-rmse:0.048136+0.000114\ttest-rmse:0.048189+0.000228 \n",
      "[20]\ttrain-rmse:0.048133+0.000116\ttest-rmse:0.048186+0.000226 \n",
      "[21]\ttrain-rmse:0.048128+0.000118\ttest-rmse:0.048183+0.000223 \n",
      "[22]\ttrain-rmse:0.048125+0.000117\ttest-rmse:0.048180+0.000224 \n",
      "[23]\ttrain-rmse:0.048123+0.000117\ttest-rmse:0.048181+0.000224 \n",
      "[24]\ttrain-rmse:0.048121+0.000117\ttest-rmse:0.048180+0.000225 \n",
      "[25]\ttrain-rmse:0.048119+0.000115\ttest-rmse:0.048178+0.000227 \n",
      "[26]\ttrain-rmse:0.048116+0.000114\ttest-rmse:0.048177+0.000228 \n",
      "[27]\ttrain-rmse:0.048114+0.000114\ttest-rmse:0.048177+0.000227 \n",
      "[28]\ttrain-rmse:0.048112+0.000115\ttest-rmse:0.048177+0.000226 \n",
      "[29]\ttrain-rmse:0.048110+0.000115\ttest-rmse:0.048177+0.000228 \n",
      "[30]\ttrain-rmse:0.048107+0.000115\ttest-rmse:0.048175+0.000228 \n",
      "[31]\ttrain-rmse:0.048104+0.000115\ttest-rmse:0.048174+0.000229 \n",
      "[32]\ttrain-rmse:0.048102+0.000114\ttest-rmse:0.048173+0.000229 \n",
      "[33]\ttrain-rmse:0.048100+0.000115\ttest-rmse:0.048171+0.000228 \n",
      "[34]\ttrain-rmse:0.048098+0.000114\ttest-rmse:0.048170+0.000227 \n",
      "[35]\ttrain-rmse:0.048096+0.000114\ttest-rmse:0.048169+0.000227 \n",
      "[36]\ttrain-rmse:0.048093+0.000114\ttest-rmse:0.048169+0.000226 \n",
      "[37]\ttrain-rmse:0.048091+0.000114\ttest-rmse:0.048169+0.000227 \n",
      "[38]\ttrain-rmse:0.048087+0.000113\ttest-rmse:0.048167+0.000229 \n",
      "[39]\ttrain-rmse:0.048085+0.000114\ttest-rmse:0.048167+0.000229 \n",
      "[40]\ttrain-rmse:0.048084+0.000113\ttest-rmse:0.048167+0.000229 \n",
      "[41]\ttrain-rmse:0.048083+0.000113\ttest-rmse:0.048167+0.000229 \n",
      "[42]\ttrain-rmse:0.048081+0.000113\ttest-rmse:0.048165+0.000229 \n",
      "[43]\ttrain-rmse:0.048079+0.000113\ttest-rmse:0.048164+0.000229 \n",
      "[44]\ttrain-rmse:0.048078+0.000114\ttest-rmse:0.048165+0.000229 \n",
      "[45]\ttrain-rmse:0.048077+0.000114\ttest-rmse:0.048164+0.000228 \n",
      "[46]\ttrain-rmse:0.048075+0.000114\ttest-rmse:0.048164+0.000229 \n",
      "[47]\ttrain-rmse:0.048073+0.000114\ttest-rmse:0.048164+0.000229 \n",
      "[48]\ttrain-rmse:0.048071+0.000114\ttest-rmse:0.048164+0.000230 \n",
      "[49]\ttrain-rmse:0.048070+0.000114\ttest-rmse:0.048165+0.000230 \n",
      "[50]\ttrain-rmse:0.048067+0.000114\ttest-rmse:0.048163+0.000231 \n",
      "[51]\ttrain-rmse:0.048065+0.000114\ttest-rmse:0.048161+0.000230 \n",
      "[52]\ttrain-rmse:0.048063+0.000113\ttest-rmse:0.048161+0.000229 \n",
      "[53]\ttrain-rmse:0.048061+0.000113\ttest-rmse:0.048161+0.000229 \n",
      "[54]\ttrain-rmse:0.048060+0.000112\ttest-rmse:0.048160+0.000229 \n",
      "[55]\ttrain-rmse:0.048058+0.000112\ttest-rmse:0.048160+0.000229 \n",
      "[56]\ttrain-rmse:0.048057+0.000113\ttest-rmse:0.048159+0.000228 \n",
      "[57]\ttrain-rmse:0.048056+0.000113\ttest-rmse:0.048159+0.000228 \n",
      "[58]\ttrain-rmse:0.048055+0.000113\ttest-rmse:0.048160+0.000228 \n",
      "[59]\ttrain-rmse:0.048053+0.000113\ttest-rmse:0.048159+0.000229 \n",
      "[60]\ttrain-rmse:0.048052+0.000114\ttest-rmse:0.048159+0.000228 \n",
      "Stopping. Best iteration:\n",
      "[57]\ttrain-rmse:0.048056+0.000113\ttest-rmse:0.048159+0.000228\n",
      "\n",
      "[1] \"btvrc\"\n",
      "[1]\ttrain-rmse:0.260099+0.000326\ttest-rmse:0.260100+0.000662 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.143775+0.000585\ttest-rmse:0.143791+0.001182 \n",
      "[3]\ttrain-rmse:0.094388+0.000892\ttest-rmse:0.094409+0.001789 \n",
      "[4]\ttrain-rmse:0.077153+0.001090\ttest-rmse:0.077186+0.002175 \n",
      "[5]\ttrain-rmse:0.072148+0.001166\ttest-rmse:0.072189+0.002301 \n",
      "[6]\ttrain-rmse:0.070809+0.001192\ttest-rmse:0.070850+0.002334 \n",
      "[7]\ttrain-rmse:0.070445+0.001198\ttest-rmse:0.070486+0.002347 \n",
      "[8]\ttrain-rmse:0.070340+0.001197\ttest-rmse:0.070385+0.002358 \n",
      "[9]\ttrain-rmse:0.070299+0.001193\ttest-rmse:0.070346+0.002360 \n",
      "[10]\ttrain-rmse:0.070278+0.001192\ttest-rmse:0.070328+0.002359 \n",
      "[11]\ttrain-rmse:0.070263+0.001196\ttest-rmse:0.070315+0.002360 \n",
      "[12]\ttrain-rmse:0.070252+0.001198\ttest-rmse:0.070306+0.002361 \n",
      "[13]\ttrain-rmse:0.070246+0.001196\ttest-rmse:0.070305+0.002361 \n",
      "[14]\ttrain-rmse:0.070237+0.001195\ttest-rmse:0.070300+0.002364 \n",
      "[15]\ttrain-rmse:0.070228+0.001198\ttest-rmse:0.070294+0.002362 \n",
      "[16]\ttrain-rmse:0.070220+0.001200\ttest-rmse:0.070285+0.002360 \n",
      "[17]\ttrain-rmse:0.070215+0.001199\ttest-rmse:0.070284+0.002361 \n",
      "[18]\ttrain-rmse:0.070212+0.001201\ttest-rmse:0.070281+0.002358 \n",
      "[19]\ttrain-rmse:0.070206+0.001201\ttest-rmse:0.070280+0.002354 \n",
      "[20]\ttrain-rmse:0.070202+0.001201\ttest-rmse:0.070278+0.002354 \n",
      "[21]\ttrain-rmse:0.070198+0.001202\ttest-rmse:0.070276+0.002352 \n",
      "[22]\ttrain-rmse:0.070194+0.001201\ttest-rmse:0.070276+0.002351 \n",
      "[23]\ttrain-rmse:0.070191+0.001202\ttest-rmse:0.070276+0.002352 \n",
      "[24]\ttrain-rmse:0.070187+0.001203\ttest-rmse:0.070273+0.002353 \n",
      "[25]\ttrain-rmse:0.070181+0.001202\ttest-rmse:0.070267+0.002353 \n",
      "[26]\ttrain-rmse:0.070177+0.001203\ttest-rmse:0.070266+0.002350 \n",
      "[27]\ttrain-rmse:0.070175+0.001203\ttest-rmse:0.070266+0.002349 \n",
      "[28]\ttrain-rmse:0.070172+0.001202\ttest-rmse:0.070263+0.002350 \n",
      "[29]\ttrain-rmse:0.070167+0.001203\ttest-rmse:0.070258+0.002352 \n",
      "[30]\ttrain-rmse:0.070164+0.001202\ttest-rmse:0.070257+0.002352 \n",
      "[31]\ttrain-rmse:0.070160+0.001202\ttest-rmse:0.070254+0.002354 \n",
      "[32]\ttrain-rmse:0.070157+0.001201\ttest-rmse:0.070255+0.002353 \n",
      "[33]\ttrain-rmse:0.070154+0.001200\ttest-rmse:0.070253+0.002354 \n",
      "[34]\ttrain-rmse:0.070152+0.001199\ttest-rmse:0.070252+0.002353 \n",
      "[35]\ttrain-rmse:0.070149+0.001200\ttest-rmse:0.070252+0.002353 \n",
      "[36]\ttrain-rmse:0.070146+0.001199\ttest-rmse:0.070253+0.002352 \n",
      "[37]\ttrain-rmse:0.070144+0.001199\ttest-rmse:0.070251+0.002353 \n",
      "[38]\ttrain-rmse:0.070140+0.001198\ttest-rmse:0.070251+0.002353 \n",
      "[39]\ttrain-rmse:0.070137+0.001198\ttest-rmse:0.070248+0.002352 \n",
      "[40]\ttrain-rmse:0.070134+0.001196\ttest-rmse:0.070249+0.002353 \n",
      "[41]\ttrain-rmse:0.070132+0.001195\ttest-rmse:0.070249+0.002352 \n",
      "[42]\ttrain-rmse:0.070131+0.001195\ttest-rmse:0.070249+0.002353 \n",
      "Stopping. Best iteration:\n",
      "[39]\ttrain-rmse:0.070137+0.001198\ttest-rmse:0.070248+0.002352\n",
      "\n",
      "[1] \"log_rnstu\"\n",
      "[1]\ttrain-rmse:0.252538+0.000007\ttest-rmse:0.252542+0.000027 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.129898+0.000015\ttest-rmse:0.129903+0.000040 \n",
      "[3]\ttrain-rmse:0.071696+0.000029\ttest-rmse:0.071700+0.000055 \n",
      "[4]\ttrain-rmse:0.046914+0.000043\ttest-rmse:0.046933+0.000085 \n",
      "[5]\ttrain-rmse:0.038254+0.000055\ttest-rmse:0.038276+0.000105 \n",
      "[6]\ttrain-rmse:0.035740+0.000062\ttest-rmse:0.035764+0.000111 \n",
      "[7]\ttrain-rmse:0.035068+0.000065\ttest-rmse:0.035095+0.000111 \n",
      "[8]\ttrain-rmse:0.034888+0.000066\ttest-rmse:0.034918+0.000111 \n",
      "[9]\ttrain-rmse:0.034835+0.000070\ttest-rmse:0.034869+0.000107 \n",
      "[10]\ttrain-rmse:0.034817+0.000067\ttest-rmse:0.034850+0.000108 \n",
      "[11]\ttrain-rmse:0.034803+0.000063\ttest-rmse:0.034838+0.000112 \n",
      "[12]\ttrain-rmse:0.034798+0.000062\ttest-rmse:0.034835+0.000113 \n",
      "[13]\ttrain-rmse:0.034793+0.000063\ttest-rmse:0.034831+0.000112 \n",
      "[14]\ttrain-rmse:0.034790+0.000063\ttest-rmse:0.034830+0.000111 \n",
      "[15]\ttrain-rmse:0.034780+0.000061\ttest-rmse:0.034822+0.000113 \n",
      "[16]\ttrain-rmse:0.034774+0.000061\ttest-rmse:0.034817+0.000112 \n",
      "[17]\ttrain-rmse:0.034770+0.000061\ttest-rmse:0.034815+0.000112 \n",
      "[18]\ttrain-rmse:0.034765+0.000060\ttest-rmse:0.034810+0.000114 \n",
      "[19]\ttrain-rmse:0.034761+0.000058\ttest-rmse:0.034807+0.000116 \n",
      "[20]\ttrain-rmse:0.034759+0.000057\ttest-rmse:0.034806+0.000117 \n",
      "[21]\ttrain-rmse:0.034755+0.000057\ttest-rmse:0.034804+0.000117 \n",
      "[22]\ttrain-rmse:0.034751+0.000059\ttest-rmse:0.034800+0.000114 \n",
      "[23]\ttrain-rmse:0.034747+0.000058\ttest-rmse:0.034796+0.000115 \n",
      "[24]\ttrain-rmse:0.034745+0.000060\ttest-rmse:0.034794+0.000113 \n",
      "[25]\ttrain-rmse:0.034744+0.000059\ttest-rmse:0.034794+0.000114 \n",
      "[26]\ttrain-rmse:0.034741+0.000061\ttest-rmse:0.034791+0.000112 \n",
      "[27]\ttrain-rmse:0.034738+0.000060\ttest-rmse:0.034789+0.000112 \n",
      "[28]\ttrain-rmse:0.034736+0.000061\ttest-rmse:0.034787+0.000112 \n",
      "[29]\ttrain-rmse:0.034734+0.000060\ttest-rmse:0.034786+0.000112 \n",
      "[30]\ttrain-rmse:0.034732+0.000060\ttest-rmse:0.034784+0.000113 \n",
      "[31]\ttrain-rmse:0.034730+0.000060\ttest-rmse:0.034783+0.000113 \n",
      "[32]\ttrain-rmse:0.034728+0.000060\ttest-rmse:0.034783+0.000113 \n",
      "[33]\ttrain-rmse:0.034725+0.000060\ttest-rmse:0.034780+0.000113 \n",
      "[34]\ttrain-rmse:0.034723+0.000059\ttest-rmse:0.034778+0.000113 \n",
      "[35]\ttrain-rmse:0.034721+0.000060\ttest-rmse:0.034778+0.000114 \n",
      "[36]\ttrain-rmse:0.034720+0.000060\ttest-rmse:0.034778+0.000113 \n",
      "[37]\ttrain-rmse:0.034718+0.000060\ttest-rmse:0.034777+0.000113 \n",
      "[38]\ttrain-rmse:0.034717+0.000060\ttest-rmse:0.034777+0.000114 \n",
      "[39]\ttrain-rmse:0.034715+0.000059\ttest-rmse:0.034776+0.000115 \n",
      "[40]\ttrain-rmse:0.034714+0.000059\ttest-rmse:0.034775+0.000115 \n",
      "[41]\ttrain-rmse:0.034711+0.000058\ttest-rmse:0.034774+0.000114 \n",
      "[42]\ttrain-rmse:0.034710+0.000058\ttest-rmse:0.034774+0.000116 \n",
      "[43]\ttrain-rmse:0.034708+0.000058\ttest-rmse:0.034773+0.000116 \n",
      "[44]\ttrain-rmse:0.034706+0.000058\ttest-rmse:0.034771+0.000116 \n",
      "[45]\ttrain-rmse:0.034705+0.000059\ttest-rmse:0.034771+0.000116 \n",
      "[46]\ttrain-rmse:0.034703+0.000058\ttest-rmse:0.034771+0.000116 \n",
      "[47]\ttrain-rmse:0.034702+0.000058\ttest-rmse:0.034771+0.000116 \n",
      "[48]\ttrain-rmse:0.034701+0.000059\ttest-rmse:0.034770+0.000115 \n",
      "[49]\ttrain-rmse:0.034700+0.000059\ttest-rmse:0.034770+0.000116 \n",
      "[50]\ttrain-rmse:0.034698+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "[51]\ttrain-rmse:0.034697+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "[52]\ttrain-rmse:0.034696+0.000057\ttest-rmse:0.034769+0.000117 \n",
      "[53]\ttrain-rmse:0.034695+0.000057\ttest-rmse:0.034769+0.000117 \n",
      "[54]\ttrain-rmse:0.034694+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "[55]\ttrain-rmse:0.034692+0.000058\ttest-rmse:0.034768+0.000116 \n",
      "[56]\ttrain-rmse:0.034691+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "[57]\ttrain-rmse:0.034690+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "[58]\ttrain-rmse:0.034689+0.000058\ttest-rmse:0.034769+0.000116 \n",
      "Stopping. Best iteration:\n",
      "[55]\ttrain-rmse:0.034692+0.000058\ttest-rmse:0.034768+0.000116\n",
      "\n",
      "[1] \"p1sdp\"\n",
      "[1]\ttrain-rmse:0.275402+0.000255\ttest-rmse:0.275408+0.000517 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.170161+0.000419\ttest-rmse:0.170173+0.000842 \n",
      "[3]\ttrain-rmse:0.131250+0.000540\ttest-rmse:0.131271+0.001082 \n",
      "[4]\ttrain-rmse:0.119535+0.000594\ttest-rmse:0.119577+0.001187 \n",
      "[5]\ttrain-rmse:0.116410+0.000608\ttest-rmse:0.116458+0.001216 \n",
      "[6]\ttrain-rmse:0.115600+0.000610\ttest-rmse:0.115662+0.001222 \n",
      "[7]\ttrain-rmse:0.115389+0.000611\ttest-rmse:0.115453+0.001227 \n",
      "[8]\ttrain-rmse:0.115328+0.000611\ttest-rmse:0.115402+0.001225 \n",
      "[9]\ttrain-rmse:0.115306+0.000612\ttest-rmse:0.115383+0.001224 \n",
      "[10]\ttrain-rmse:0.115293+0.000611\ttest-rmse:0.115371+0.001225 \n",
      "[11]\ttrain-rmse:0.115284+0.000611\ttest-rmse:0.115368+0.001222 \n",
      "[12]\ttrain-rmse:0.115276+0.000611\ttest-rmse:0.115368+0.001223 \n",
      "[13]\ttrain-rmse:0.115264+0.000611\ttest-rmse:0.115371+0.001226 \n",
      "[14]\ttrain-rmse:0.115253+0.000608\ttest-rmse:0.115367+0.001226 \n",
      "[15]\ttrain-rmse:0.115243+0.000607\ttest-rmse:0.115366+0.001227 \n",
      "[16]\ttrain-rmse:0.115236+0.000610\ttest-rmse:0.115361+0.001230 \n",
      "[17]\ttrain-rmse:0.115227+0.000611\ttest-rmse:0.115364+0.001230 \n",
      "[18]\ttrain-rmse:0.115222+0.000611\ttest-rmse:0.115360+0.001228 \n",
      "[19]\ttrain-rmse:0.115215+0.000615\ttest-rmse:0.115359+0.001227 \n",
      "[20]\ttrain-rmse:0.115205+0.000615\ttest-rmse:0.115359+0.001221 \n",
      "[21]\ttrain-rmse:0.115197+0.000616\ttest-rmse:0.115360+0.001220 \n",
      "[22]\ttrain-rmse:0.115190+0.000618\ttest-rmse:0.115369+0.001226 \n",
      "Stopping. Best iteration:\n",
      "[19]\ttrain-rmse:0.115215+0.000615\ttest-rmse:0.115359+0.001227\n",
      "\n",
      "[1] \"p2shp\"\n",
      "[1]\ttrain-rmse:0.313132+0.000999\ttest-rmse:0.313141+0.002020 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.225975+0.001369\ttest-rmse:0.225997+0.002763 \n",
      "[3]\ttrain-rmse:0.198245+0.001533\ttest-rmse:0.198281+0.003136 \n",
      "[4]\ttrain-rmse:0.190617+0.001593\ttest-rmse:0.190664+0.003234 \n",
      "[5]\ttrain-rmse:0.188647+0.001606\ttest-rmse:0.188700+0.003271 \n",
      "[6]\ttrain-rmse:0.188118+0.001628\ttest-rmse:0.188180+0.003245 \n",
      "[7]\ttrain-rmse:0.187958+0.001608\ttest-rmse:0.188041+0.003246 \n",
      "[8]\ttrain-rmse:0.187906+0.001611\ttest-rmse:0.187988+0.003238 \n",
      "[9]\ttrain-rmse:0.187881+0.001606\ttest-rmse:0.187977+0.003239 \n",
      "[10]\ttrain-rmse:0.187844+0.001607\ttest-rmse:0.187954+0.003250 \n",
      "[11]\ttrain-rmse:0.187826+0.001598\ttest-rmse:0.187945+0.003258 \n",
      "[12]\ttrain-rmse:0.187804+0.001595\ttest-rmse:0.187939+0.003253 \n",
      "[13]\ttrain-rmse:0.187793+0.001592\ttest-rmse:0.187939+0.003251 \n",
      "[14]\ttrain-rmse:0.187783+0.001593\ttest-rmse:0.187936+0.003250 \n",
      "[15]\ttrain-rmse:0.187775+0.001594\ttest-rmse:0.187932+0.003245 \n",
      "[16]\ttrain-rmse:0.187743+0.001603\ttest-rmse:0.187912+0.003232 \n",
      "[17]\ttrain-rmse:0.187732+0.001597\ttest-rmse:0.187905+0.003241 \n",
      "[18]\ttrain-rmse:0.187721+0.001594\ttest-rmse:0.187898+0.003244 \n",
      "[19]\ttrain-rmse:0.187709+0.001597\ttest-rmse:0.187897+0.003241 \n",
      "[20]\ttrain-rmse:0.187696+0.001603\ttest-rmse:0.187893+0.003232 \n",
      "[21]\ttrain-rmse:0.187684+0.001602\ttest-rmse:0.187886+0.003229 \n",
      "[22]\ttrain-rmse:0.187669+0.001601\ttest-rmse:0.187890+0.003238 \n",
      "[23]\ttrain-rmse:0.187665+0.001602\ttest-rmse:0.187893+0.003241 \n",
      "[24]\ttrain-rmse:0.187657+0.001603\ttest-rmse:0.187897+0.003243 \n",
      "Stopping. Best iteration:\n",
      "[21]\ttrain-rmse:0.187684+0.001602\ttest-rmse:0.187886+0.003229\n",
      "\n",
      "[1] \"p3rop\"\n",
      "[1]\ttrain-rmse:0.323419+0.000553\ttest-rmse:0.323422+0.001113 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.240099+0.000740\ttest-rmse:0.240117+0.001515 \n",
      "[3]\ttrain-rmse:0.214234+0.000858\ttest-rmse:0.214269+0.001678 \n",
      "[4]\ttrain-rmse:0.207226+0.000884\ttest-rmse:0.207269+0.001727 \n",
      "[5]\ttrain-rmse:0.205419+0.000891\ttest-rmse:0.205477+0.001738 \n",
      "[6]\ttrain-rmse:0.204936+0.000901\ttest-rmse:0.205001+0.001730 \n",
      "[7]\ttrain-rmse:0.204783+0.000903\ttest-rmse:0.204858+0.001726 \n",
      "[8]\ttrain-rmse:0.204728+0.000906\ttest-rmse:0.204812+0.001727 \n",
      "[9]\ttrain-rmse:0.204704+0.000909\ttest-rmse:0.204796+0.001730 \n",
      "[10]\ttrain-rmse:0.204682+0.000905\ttest-rmse:0.204781+0.001741 \n",
      "[11]\ttrain-rmse:0.204671+0.000904\ttest-rmse:0.204778+0.001736 \n",
      "[12]\ttrain-rmse:0.204658+0.000901\ttest-rmse:0.204770+0.001740 \n",
      "[13]\ttrain-rmse:0.204643+0.000906\ttest-rmse:0.204772+0.001732 \n",
      "[14]\ttrain-rmse:0.204631+0.000907\ttest-rmse:0.204770+0.001728 \n",
      "[15]\ttrain-rmse:0.204613+0.000906\ttest-rmse:0.204764+0.001727 \n",
      "[16]\ttrain-rmse:0.204607+0.000905\ttest-rmse:0.204762+0.001726 \n",
      "[17]\ttrain-rmse:0.204590+0.000900\ttest-rmse:0.204749+0.001741 \n",
      "[18]\ttrain-rmse:0.204581+0.000902\ttest-rmse:0.204746+0.001742 \n",
      "[19]\ttrain-rmse:0.204571+0.000901\ttest-rmse:0.204745+0.001739 \n",
      "[20]\ttrain-rmse:0.204561+0.000898\ttest-rmse:0.204746+0.001742 \n",
      "[21]\ttrain-rmse:0.204552+0.000897\ttest-rmse:0.204747+0.001737 \n",
      "[22]\ttrain-rmse:0.204539+0.000899\ttest-rmse:0.204741+0.001732 \n",
      "[23]\ttrain-rmse:0.204533+0.000900\ttest-rmse:0.204739+0.001730 \n",
      "[24]\ttrain-rmse:0.204524+0.000895\ttest-rmse:0.204733+0.001737 \n",
      "[25]\ttrain-rmse:0.204516+0.000895\ttest-rmse:0.204733+0.001736 \n",
      "[26]\ttrain-rmse:0.204510+0.000894\ttest-rmse:0.204730+0.001741 \n",
      "[27]\ttrain-rmse:0.204501+0.000895\ttest-rmse:0.204731+0.001745 \n",
      "[28]\ttrain-rmse:0.204494+0.000893\ttest-rmse:0.204733+0.001743 \n",
      "[29]\ttrain-rmse:0.204483+0.000892\ttest-rmse:0.204730+0.001745 \n",
      "Stopping. Best iteration:\n",
      "[26]\ttrain-rmse:0.204510+0.000894\ttest-rmse:0.204730+0.001741\n",
      "\n",
      "[1] \"stalter_p1sdp\"\n",
      "[1]\ttrain-rmse:0.251737+0.000011\ttest-rmse:0.251738+0.000021 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.128428+0.000023\ttest-rmse:0.128429+0.000045 \n",
      "[3]\ttrain-rmse:0.069080+0.000043\ttest-rmse:0.069085+0.000080 \n",
      "[4]\ttrain-rmse:0.042910+0.000070\ttest-rmse:0.042922+0.000128 \n",
      "[5]\ttrain-rmse:0.033290+0.000091\ttest-rmse:0.033308+0.000167 \n",
      "[6]\ttrain-rmse:0.030403+0.000093\ttest-rmse:0.030422+0.000196 \n",
      "[7]\ttrain-rmse:0.029628+0.000101\ttest-rmse:0.029648+0.000196 \n",
      "[8]\ttrain-rmse:0.029427+0.000101\ttest-rmse:0.029451+0.000199 \n",
      "[9]\ttrain-rmse:0.029371+0.000101\ttest-rmse:0.029400+0.000200 \n",
      "[10]\ttrain-rmse:0.029353+0.000101\ttest-rmse:0.029384+0.000202 \n",
      "[11]\ttrain-rmse:0.029344+0.000101\ttest-rmse:0.029379+0.000200 \n",
      "[12]\ttrain-rmse:0.029337+0.000102\ttest-rmse:0.029376+0.000199 \n",
      "[13]\ttrain-rmse:0.029333+0.000104\ttest-rmse:0.029372+0.000198 \n",
      "[14]\ttrain-rmse:0.029328+0.000102\ttest-rmse:0.029368+0.000199 \n",
      "[15]\ttrain-rmse:0.029325+0.000102\ttest-rmse:0.029366+0.000199 \n",
      "[16]\ttrain-rmse:0.029322+0.000103\ttest-rmse:0.029364+0.000200 \n",
      "[17]\ttrain-rmse:0.029313+0.000103\ttest-rmse:0.029368+0.000209 \n",
      "[18]\ttrain-rmse:0.029310+0.000104\ttest-rmse:0.029367+0.000208 \n",
      "[19]\ttrain-rmse:0.029308+0.000104\ttest-rmse:0.029365+0.000208 \n",
      "Stopping. Best iteration:\n",
      "[16]\ttrain-rmse:0.029322+0.000103\ttest-rmse:0.029364+0.000200\n",
      "\n",
      "[1] \"stalter_p2shp\"\n",
      "[1]\ttrain-rmse:0.254871+0.000032\ttest-rmse:0.254868+0.000059 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.134409+0.000059\ttest-rmse:0.134410+0.000118 \n",
      "[3]\ttrain-rmse:0.079616+0.000104\ttest-rmse:0.079632+0.000202 \n",
      "[4]\ttrain-rmse:0.058353+0.000142\ttest-rmse:0.058376+0.000276 \n",
      "[5]\ttrain-rmse:0.051678+0.000161\ttest-rmse:0.051710+0.000309 \n",
      "[6]\ttrain-rmse:0.049858+0.000165\ttest-rmse:0.049897+0.000323 \n",
      "[7]\ttrain-rmse:0.049390+0.000167\ttest-rmse:0.049433+0.000326 \n",
      "[8]\ttrain-rmse:0.049262+0.000167\ttest-rmse:0.049308+0.000326 \n",
      "[9]\ttrain-rmse:0.049228+0.000167\ttest-rmse:0.049275+0.000327 \n",
      "[10]\ttrain-rmse:0.049214+0.000169\ttest-rmse:0.049264+0.000324 \n",
      "[11]\ttrain-rmse:0.049208+0.000167\ttest-rmse:0.049258+0.000329 \n",
      "[12]\ttrain-rmse:0.049203+0.000164\ttest-rmse:0.049255+0.000330 \n",
      "[13]\ttrain-rmse:0.049198+0.000166\ttest-rmse:0.049254+0.000328 \n",
      "[14]\ttrain-rmse:0.049196+0.000165\ttest-rmse:0.049253+0.000328 \n",
      "[15]\ttrain-rmse:0.049192+0.000167\ttest-rmse:0.049252+0.000325 \n",
      "[16]\ttrain-rmse:0.049187+0.000170\ttest-rmse:0.049249+0.000323 \n",
      "[17]\ttrain-rmse:0.049182+0.000169\ttest-rmse:0.049248+0.000323 \n",
      "[18]\ttrain-rmse:0.049179+0.000168\ttest-rmse:0.049246+0.000323 \n",
      "[19]\ttrain-rmse:0.049177+0.000170\ttest-rmse:0.049245+0.000322 \n",
      "[20]\ttrain-rmse:0.049173+0.000167\ttest-rmse:0.049244+0.000325 \n",
      "[21]\ttrain-rmse:0.049166+0.000168\ttest-rmse:0.049240+0.000324 \n",
      "[22]\ttrain-rmse:0.049162+0.000169\ttest-rmse:0.049237+0.000322 \n",
      "[23]\ttrain-rmse:0.049159+0.000168\ttest-rmse:0.049236+0.000322 \n",
      "[24]\ttrain-rmse:0.049152+0.000169\ttest-rmse:0.049233+0.000323 \n",
      "[25]\ttrain-rmse:0.049148+0.000168\ttest-rmse:0.049231+0.000324 \n",
      "[26]\ttrain-rmse:0.049143+0.000167\ttest-rmse:0.049228+0.000325 \n",
      "[27]\ttrain-rmse:0.049139+0.000165\ttest-rmse:0.049224+0.000328 \n",
      "[28]\ttrain-rmse:0.049135+0.000164\ttest-rmse:0.049225+0.000327 \n",
      "[29]\ttrain-rmse:0.049133+0.000165\ttest-rmse:0.049225+0.000328 \n",
      "[30]\ttrain-rmse:0.049131+0.000164\ttest-rmse:0.049225+0.000328 \n",
      "Stopping. Best iteration:\n",
      "[27]\ttrain-rmse:0.049139+0.000165\ttest-rmse:0.049224+0.000328\n",
      "\n",
      "[1] \"stalter_p3rop\"\n",
      "[1]\ttrain-rmse:0.255696+0.000087\ttest-rmse:0.255704+0.000168 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 3 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:0.136015+0.000162\ttest-rmse:0.136025+0.000317 \n",
      "[3]\ttrain-rmse:0.082329+0.000266\ttest-rmse:0.082341+0.000533 \n",
      "[4]\ttrain-rmse:0.062017+0.000355\ttest-rmse:0.062028+0.000704 \n",
      "[5]\ttrain-rmse:0.055786+0.000396\ttest-rmse:0.055804+0.000777 \n",
      "[6]\ttrain-rmse:0.054107+0.000403\ttest-rmse:0.054129+0.000804 \n",
      "[7]\ttrain-rmse:0.053670+0.000406\ttest-rmse:0.053692+0.000811 \n",
      "[8]\ttrain-rmse:0.053554+0.000404\ttest-rmse:0.053580+0.000814 \n",
      "[9]\ttrain-rmse:0.053522+0.000403\ttest-rmse:0.053550+0.000814 \n",
      "[10]\ttrain-rmse:0.053509+0.000402\ttest-rmse:0.053539+0.000813 \n",
      "[11]\ttrain-rmse:0.053503+0.000404\ttest-rmse:0.053534+0.000811 \n",
      "[12]\ttrain-rmse:0.053499+0.000403\ttest-rmse:0.053531+0.000813 \n",
      "[13]\ttrain-rmse:0.053493+0.000402\ttest-rmse:0.053527+0.000814 \n",
      "[14]\ttrain-rmse:0.053488+0.000402\ttest-rmse:0.053525+0.000813 \n",
      "[15]\ttrain-rmse:0.053484+0.000401\ttest-rmse:0.053522+0.000815 \n",
      "[16]\ttrain-rmse:0.053480+0.000399\ttest-rmse:0.053519+0.000815 \n",
      "[17]\ttrain-rmse:0.053476+0.000401\ttest-rmse:0.053517+0.000815 \n",
      "[18]\ttrain-rmse:0.053472+0.000402\ttest-rmse:0.053516+0.000812 \n",
      "[19]\ttrain-rmse:0.053470+0.000402\ttest-rmse:0.053515+0.000814 \n",
      "[20]\ttrain-rmse:0.053467+0.000403\ttest-rmse:0.053514+0.000811 \n",
      "[21]\ttrain-rmse:0.053464+0.000404\ttest-rmse:0.053511+0.000812 \n",
      "[22]\ttrain-rmse:0.053459+0.000404\ttest-rmse:0.053505+0.000812 \n",
      "[23]\ttrain-rmse:0.053456+0.000405\ttest-rmse:0.053504+0.000812 \n",
      "[24]\ttrain-rmse:0.053452+0.000403\ttest-rmse:0.053504+0.000811 \n",
      "[25]\ttrain-rmse:0.053450+0.000403\ttest-rmse:0.053502+0.000811 \n",
      "[26]\ttrain-rmse:0.053448+0.000403\ttest-rmse:0.053502+0.000812 \n",
      "[27]\ttrain-rmse:0.053446+0.000403\ttest-rmse:0.053502+0.000812 \n",
      "[28]\ttrain-rmse:0.053444+0.000404\ttest-rmse:0.053502+0.000812 \n",
      "[29]\ttrain-rmse:0.053442+0.000404\ttest-rmse:0.053501+0.000811 \n",
      "[30]\ttrain-rmse:0.053439+0.000405\ttest-rmse:0.053500+0.000810 \n",
      "[31]\ttrain-rmse:0.053437+0.000404\ttest-rmse:0.053500+0.000812 \n",
      "[32]\ttrain-rmse:0.053435+0.000403\ttest-rmse:0.053499+0.000812 \n",
      "[33]\ttrain-rmse:0.053433+0.000404\ttest-rmse:0.053499+0.000812 \n",
      "[34]\ttrain-rmse:0.053431+0.000404\ttest-rmse:0.053499+0.000813 \n",
      "[35]\ttrain-rmse:0.053428+0.000404\ttest-rmse:0.053498+0.000812 \n",
      "[36]\ttrain-rmse:0.053426+0.000404\ttest-rmse:0.053498+0.000811 \n",
      "[37]\ttrain-rmse:0.053425+0.000405\ttest-rmse:0.053498+0.000813 \n",
      "[38]\ttrain-rmse:0.053422+0.000404\ttest-rmse:0.053498+0.000813 \n",
      "[39]\ttrain-rmse:0.053420+0.000403\ttest-rmse:0.053498+0.000813 \n",
      "[40]\ttrain-rmse:0.053418+0.000403\ttest-rmse:0.053499+0.000813 \n",
      "[41]\ttrain-rmse:0.053415+0.000403\ttest-rmse:0.053499+0.000812 \n",
      "Stopping. Best iteration:\n",
      "[38]\ttrain-rmse:0.053422+0.000404\ttest-rmse:0.053498+0.000813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_xgr <- colnames(\n",
    "    df %>%\n",
    "        select(\n",
    "            log_mcbgv, \n",
    "            log_pgt2kmt, \n",
    "            log_pgt1hafh, \n",
    "            log_pnchd, \n",
    "            btvrc, \n",
    "            log_rnstu,\n",
    "            p1sdp,\n",
    "            p2shp,\n",
    "            p3rop,\n",
    "            stalter_p1sdp,\n",
    "            stalter_p2shp,\n",
    "            stalter_p3rop\n",
    "        )\n",
    ")\n",
    "\n",
    "foreach(i = 1:length(cols_to_xgr), .combine = cbind) %do% \n",
    "    XGresidualizer(df[[cols_to_xgr[i]]], cols_to_xgr[i]) -> xg.residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldvs <- c('log_mcbgv.r', 'log_pgt2kmt.r', 'log_pgt1hafh.r', 'log_pnchd.r', 'btvrc.r', 'log_rnstu.r')\n",
    "f1 <- ' p1sdp.r + p2shp.r + p3rop.r '  \n",
    "f2 <- ' p1sdp.r + p2shp.r + p3rop.r + stalter_p1sdp.r + stalter_p2shp.r + stalter_p3rop.r '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df %>%\n",
    "    select(date, key, n, cluster) %>%\n",
    "    bind_cols(xg.residuals) -> df.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coefs <- function(dv, vars, ...) {\n",
    "    lhs <- str_c(dv, ' ~ ')\n",
    "    rhs <- str_c(vars, '| key + date | 0 | cluster')\n",
    "    form <- as.formula(str_c(lhs, rhs))\n",
    "    model <- felm(form, df.r, weights = df.r$n, ...)\n",
    "    \n",
    "    as.data.frame(summary(model)$coef) %>%\n",
    "        mutate(var   = rownames(.),\n",
    "               dv    = dv,\n",
    "               model = ifelse(vars == f1, 'base', 'ap')) %>%\n",
    "        filter(!str_detect(var, '[pPtT][rRmM][cCaA][pPxX]')) %>%\n",
    "        select(7, 6, 5, 1, 2, 3, 4) -> out\n",
    "    \n",
    "    colnames(out) <- c('model', 'dv', 'var', 'estimate', 'se', 't', 'p-val')\n",
    "    return(out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreach(dv = ldvs, .combine = rbind) %:% \n",
    "    foreach(f = c(f1, f2), .combine = rbind) %dopar%\n",
    "    get_coefs(dv, f) -> coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=15, repr.plot.height=10)\n",
    "pn <- c('p1sdp' = \"Initial Policies\",\n",
    "        'p2shp' = \"Shelter-in-place\",\n",
    "        'p3rop' = \"Reopening\",\n",
    "        'a'     = 'ego state policy',\n",
    "        'b'     = 'alter state policy')\n",
    "\n",
    "coefs %>%\n",
    "    mutate(dv   = str_sub(dv, 1, -3),\n",
    "           var  = str_sub(var, 1, -3),\n",
    "           type = ifelse(dv == 'log_rnstu' | dv == 'btvrc', 'fb', 'sg'), \n",
    "           type = factor(type, levels = c('sg', 'fb'))) %>%\n",
    "    mutate(vtype = ifelse(str_detect(var, 'stalter'), 'b', 'a'),\n",
    "           var   = str_replace(var, 'stalter_', ''),\n",
    "           model = factor(model, levels = c('base', 'ap')),\n",
    "           dv = factor(dv, levels = c('log_mcbgv', 'log_pgt2kmt', 'log_pgt1hafh', 'log_pnchd', 'btvrc', 'log_rnstu'))) %>%\n",
    "    ggplot(aes(x = model, y = estimate, color = dv, shape = type)) +\n",
    "    geom_hline(aes(yintercept = 0), linetype = 2) +\n",
    "    geom_point(position = position_dodge(width = .5), size = 2) + \n",
    "    geom_linerange(aes(ymin = estimate - 1.98 * se, ymax = estimate + 1.98 * se), \n",
    "                   position = position_dodge(width = .5), size = .5) +\n",
    "    facet_grid(vtype~var, scales = 'free_y', labeller = as_labeller(pn)) +\n",
    "    xlab('') + \n",
    "    ylab('') +\n",
    "    scale_shape(guide = 'none') + \n",
    "    scale_color_d3() +\n",
    "    labs(color = \"Outcome\", shape = '') +\n",
    "    theme_light() +\n",
    "    theme(text = element_text(size=20),\n",
    "          legend.position = 'bottom') -> p\n",
    "\n",
    "ggsave('/home/mfzhao/SI_plots/rc1_fb.pdf', p, device = 'pdf', width = 6.5, height = 6.5, scale = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 122  1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>date</th></tr>\n",
       "\t<tr><th scope=col>&lt;date&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>2020-03-01</td></tr>\n",
       "\t<tr><td>2020-03-02</td></tr>\n",
       "\t<tr><td>2020-03-03</td></tr>\n",
       "\t<tr><td>2020-03-04</td></tr>\n",
       "\t<tr><td>2020-03-05</td></tr>\n",
       "\t<tr><td>2020-03-06</td></tr>\n",
       "\t<tr><td>2020-03-07</td></tr>\n",
       "\t<tr><td>2020-03-08</td></tr>\n",
       "\t<tr><td>2020-03-09</td></tr>\n",
       "\t<tr><td>2020-03-10</td></tr>\n",
       "\t<tr><td>2020-03-11</td></tr>\n",
       "\t<tr><td>2020-03-12</td></tr>\n",
       "\t<tr><td>2020-03-13</td></tr>\n",
       "\t<tr><td>2020-03-14</td></tr>\n",
       "\t<tr><td>2020-03-15</td></tr>\n",
       "\t<tr><td>2020-03-16</td></tr>\n",
       "\t<tr><td>2020-03-17</td></tr>\n",
       "\t<tr><td>2020-03-18</td></tr>\n",
       "\t<tr><td>2020-03-19</td></tr>\n",
       "\t<tr><td>2020-03-20</td></tr>\n",
       "\t<tr><td>2020-03-21</td></tr>\n",
       "\t<tr><td>2020-03-22</td></tr>\n",
       "\t<tr><td>2020-03-23</td></tr>\n",
       "\t<tr><td>2020-03-24</td></tr>\n",
       "\t<tr><td>2020-03-25</td></tr>\n",
       "\t<tr><td>2020-03-26</td></tr>\n",
       "\t<tr><td>2020-03-27</td></tr>\n",
       "\t<tr><td>2020-03-28</td></tr>\n",
       "\t<tr><td>2020-03-29</td></tr>\n",
       "\t<tr><td>2020-03-30</td></tr>\n",
       "\t<tr><td></td></tr>\n",
       "\t<tr><td>2020-06-01</td></tr>\n",
       "\t<tr><td>2020-06-02</td></tr>\n",
       "\t<tr><td>2020-06-03</td></tr>\n",
       "\t<tr><td>2020-06-04</td></tr>\n",
       "\t<tr><td>2020-06-05</td></tr>\n",
       "\t<tr><td>2020-06-06</td></tr>\n",
       "\t<tr><td>2020-06-07</td></tr>\n",
       "\t<tr><td>2020-06-08</td></tr>\n",
       "\t<tr><td>2020-06-09</td></tr>\n",
       "\t<tr><td>2020-06-10</td></tr>\n",
       "\t<tr><td>2020-06-11</td></tr>\n",
       "\t<tr><td>2020-06-12</td></tr>\n",
       "\t<tr><td>2020-06-13</td></tr>\n",
       "\t<tr><td>2020-06-14</td></tr>\n",
       "\t<tr><td>2020-06-15</td></tr>\n",
       "\t<tr><td>2020-06-16</td></tr>\n",
       "\t<tr><td>2020-06-17</td></tr>\n",
       "\t<tr><td>2020-06-18</td></tr>\n",
       "\t<tr><td>2020-06-19</td></tr>\n",
       "\t<tr><td>2020-06-20</td></tr>\n",
       "\t<tr><td>2020-06-21</td></tr>\n",
       "\t<tr><td>2020-06-22</td></tr>\n",
       "\t<tr><td>2020-06-23</td></tr>\n",
       "\t<tr><td>2020-06-24</td></tr>\n",
       "\t<tr><td>2020-06-25</td></tr>\n",
       "\t<tr><td>2020-06-26</td></tr>\n",
       "\t<tr><td>2020-06-27</td></tr>\n",
       "\t<tr><td>2020-06-28</td></tr>\n",
       "\t<tr><td>2020-06-29</td></tr>\n",
       "\t<tr><td>2020-06-30</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 122  1\n",
       "\\begin{tabular}{l}\n",
       " date\\\\\n",
       " <date>\\\\\n",
       "\\hline\n",
       "\t 2020-03-01\\\\\n",
       "\t 2020-03-02\\\\\n",
       "\t 2020-03-03\\\\\n",
       "\t 2020-03-04\\\\\n",
       "\t 2020-03-05\\\\\n",
       "\t 2020-03-06\\\\\n",
       "\t 2020-03-07\\\\\n",
       "\t 2020-03-08\\\\\n",
       "\t 2020-03-09\\\\\n",
       "\t 2020-03-10\\\\\n",
       "\t 2020-03-11\\\\\n",
       "\t 2020-03-12\\\\\n",
       "\t 2020-03-13\\\\\n",
       "\t 2020-03-14\\\\\n",
       "\t 2020-03-15\\\\\n",
       "\t 2020-03-16\\\\\n",
       "\t 2020-03-17\\\\\n",
       "\t 2020-03-18\\\\\n",
       "\t 2020-03-19\\\\\n",
       "\t 2020-03-20\\\\\n",
       "\t 2020-03-21\\\\\n",
       "\t 2020-03-22\\\\\n",
       "\t 2020-03-23\\\\\n",
       "\t 2020-03-24\\\\\n",
       "\t 2020-03-25\\\\\n",
       "\t 2020-03-26\\\\\n",
       "\t 2020-03-27\\\\\n",
       "\t 2020-03-28\\\\\n",
       "\t 2020-03-29\\\\\n",
       "\t 2020-03-30\\\\\n",
       "\t \\\\\n",
       "\t 2020-06-01\\\\\n",
       "\t 2020-06-02\\\\\n",
       "\t 2020-06-03\\\\\n",
       "\t 2020-06-04\\\\\n",
       "\t 2020-06-05\\\\\n",
       "\t 2020-06-06\\\\\n",
       "\t 2020-06-07\\\\\n",
       "\t 2020-06-08\\\\\n",
       "\t 2020-06-09\\\\\n",
       "\t 2020-06-10\\\\\n",
       "\t 2020-06-11\\\\\n",
       "\t 2020-06-12\\\\\n",
       "\t 2020-06-13\\\\\n",
       "\t 2020-06-14\\\\\n",
       "\t 2020-06-15\\\\\n",
       "\t 2020-06-16\\\\\n",
       "\t 2020-06-17\\\\\n",
       "\t 2020-06-18\\\\\n",
       "\t 2020-06-19\\\\\n",
       "\t 2020-06-20\\\\\n",
       "\t 2020-06-21\\\\\n",
       "\t 2020-06-22\\\\\n",
       "\t 2020-06-23\\\\\n",
       "\t 2020-06-24\\\\\n",
       "\t 2020-06-25\\\\\n",
       "\t 2020-06-26\\\\\n",
       "\t 2020-06-27\\\\\n",
       "\t 2020-06-28\\\\\n",
       "\t 2020-06-29\\\\\n",
       "\t 2020-06-30\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 122  1\n",
       "\n",
       "| date &lt;date&gt; |\n",
       "|---|\n",
       "| 2020-03-01 |\n",
       "| 2020-03-02 |\n",
       "| 2020-03-03 |\n",
       "| 2020-03-04 |\n",
       "| 2020-03-05 |\n",
       "| 2020-03-06 |\n",
       "| 2020-03-07 |\n",
       "| 2020-03-08 |\n",
       "| 2020-03-09 |\n",
       "| 2020-03-10 |\n",
       "| 2020-03-11 |\n",
       "| 2020-03-12 |\n",
       "| 2020-03-13 |\n",
       "| 2020-03-14 |\n",
       "| 2020-03-15 |\n",
       "| 2020-03-16 |\n",
       "| 2020-03-17 |\n",
       "| 2020-03-18 |\n",
       "| 2020-03-19 |\n",
       "| 2020-03-20 |\n",
       "| 2020-03-21 |\n",
       "| 2020-03-22 |\n",
       "| 2020-03-23 |\n",
       "| 2020-03-24 |\n",
       "| 2020-03-25 |\n",
       "| 2020-03-26 |\n",
       "| 2020-03-27 |\n",
       "| 2020-03-28 |\n",
       "| 2020-03-29 |\n",
       "| 2020-03-30 |\n",
       "|  |\n",
       "| 2020-06-01 |\n",
       "| 2020-06-02 |\n",
       "| 2020-06-03 |\n",
       "| 2020-06-04 |\n",
       "| 2020-06-05 |\n",
       "| 2020-06-06 |\n",
       "| 2020-06-07 |\n",
       "| 2020-06-08 |\n",
       "| 2020-06-09 |\n",
       "| 2020-06-10 |\n",
       "| 2020-06-11 |\n",
       "| 2020-06-12 |\n",
       "| 2020-06-13 |\n",
       "| 2020-06-14 |\n",
       "| 2020-06-15 |\n",
       "| 2020-06-16 |\n",
       "| 2020-06-17 |\n",
       "| 2020-06-18 |\n",
       "| 2020-06-19 |\n",
       "| 2020-06-20 |\n",
       "| 2020-06-21 |\n",
       "| 2020-06-22 |\n",
       "| 2020-06-23 |\n",
       "| 2020-06-24 |\n",
       "| 2020-06-25 |\n",
       "| 2020-06-26 |\n",
       "| 2020-06-27 |\n",
       "| 2020-06-28 |\n",
       "| 2020-06-29 |\n",
       "| 2020-06-30 |\n",
       "\n"
      ],
      "text/plain": [
       "    date      \n",
       "1   2020-03-01\n",
       "2   2020-03-02\n",
       "3   2020-03-03\n",
       "4   2020-03-04\n",
       "5   2020-03-05\n",
       "6   2020-03-06\n",
       "7   2020-03-07\n",
       "8   2020-03-08\n",
       "9   2020-03-09\n",
       "10  2020-03-10\n",
       "11  2020-03-11\n",
       "12  2020-03-12\n",
       "13  2020-03-13\n",
       "14  2020-03-14\n",
       "15  2020-03-15\n",
       "16  2020-03-16\n",
       "17  2020-03-17\n",
       "18  2020-03-18\n",
       "19  2020-03-19\n",
       "20  2020-03-20\n",
       "21  2020-03-21\n",
       "22  2020-03-22\n",
       "23  2020-03-23\n",
       "24  2020-03-24\n",
       "25  2020-03-25\n",
       "26  2020-03-26\n",
       "27  2020-03-27\n",
       "28  2020-03-28\n",
       "29  2020-03-29\n",
       "30  2020-03-30\n",
       "            \n",
       "93  2020-06-01\n",
       "94  2020-06-02\n",
       "95  2020-06-03\n",
       "96  2020-06-04\n",
       "97  2020-06-05\n",
       "98  2020-06-06\n",
       "99  2020-06-07\n",
       "100 2020-06-08\n",
       "101 2020-06-09\n",
       "102 2020-06-10\n",
       "103 2020-06-11\n",
       "104 2020-06-12\n",
       "105 2020-06-13\n",
       "106 2020-06-14\n",
       "107 2020-06-15\n",
       "108 2020-06-16\n",
       "109 2020-06-17\n",
       "110 2020-06-18\n",
       "111 2020-06-19\n",
       "112 2020-06-20\n",
       "113 2020-06-21\n",
       "114 2020-06-22\n",
       "115 2020-06-23\n",
       "116 2020-06-24\n",
       "117 2020-06-25\n",
       "118 2020-06-26\n",
       "119 2020-06-27\n",
       "120 2020-06-28\n",
       "121 2020-06-29\n",
       "122 2020-06-30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df %>%\n",
    "    select(date) %>%\n",
    "    distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.6",
   "language": "R",
   "name": "ir36"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
